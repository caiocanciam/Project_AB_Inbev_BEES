from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col, coalesce
import pytz
from datetime import datetime


# ------------------------------
# Silver Layer: Data cleaning & standardization
# ------------------------------
def silver_layer():
    spark = SparkSession.builder \
    .appName("silver_layer") \
    .config("spark.driver.bindAddress", "0.0.0.0") \
    .getOrCreate()

    input_url = "/opt/airflow/layers/bronze/"
    output_path = "/opt/airflow/layers/silver/"
    
    todos_dados = []
    
    for arquivo in os.listdir(input_url):
        if arquivo.endswith(".json"): # Get only .json
            caminho = os.path.join(input_url, arquivo)
            with open(caminho, "r", encoding="utf-8") as f:
                dados = json.load(f)
                todos_dados.append(dados)
    
    # Read raw data from bronze layer
    df_spark = spark.read.option("multiline", "true").json(input_url)
    
    # Force all columns to string for consistency
    df_spark = df_spark.select([F.col(c).cast("string").alias(c) for c in df_spark.columns])
    
    # Create location column (city or state_province)
    df_silver = df_spark.withColumn(
        "brewery_location",
        coalesce(col("city"), col("state_province"))
    )
    
    # Remove duplicates by brewery ID
    df_silver = df_silver.dropDuplicates(["id"])
    
    # Add execution date column
    date = str(datetime.now(pytz.timezone("Brazil/East"))).split(" ")[0]
    df_silver = df_silver.withColumn("exec_date", F.to_date(F.lit(date)))
    
    # Save silver layer, partitioned by location
    df_silver.write \
        .mode("overwrite") \
        .partitionBy("brewery_location") \
        .parquet(output_path)
    
